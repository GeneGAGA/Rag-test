{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock - End to end example\n",
    "\n",
    "This notebook provides sample code for building an empty OpenSearch Serverless (OSS) index, Amazon Bedrock knowledge base and ingest documents into the index.\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "A data pipeline that ingests documents (typically stored in Amazon S3) into a knowledge base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "- Load the documents into the knowledge base by connecting your s3 bucket (data source). \n",
    "- Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Steps: \n",
    "- Create Amazon Bedrock Knowledge Base execution role with necessary policies for accessing data from S3 and writing embeddings into OSS.\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Download documents\n",
    "- Create Amazon Bedrock knowledge base\n",
    "- Create a data source within knowledge base which will connect to Amazon S3\n",
    "- Start an ingestion job using KB APIs which will read data from s3, chunk it, convert chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy and manage the data pipeline.\n",
    "\n",
    "Once the data is available in the Bedrock Knowledge Base then a question answering application can be built using the Knowledge Base APIs provided by Amazon Bedrock in following notebooks in the same folder. \n",
    "- [1_managed-rag-kb-retrieve-generate-api.ipynb](./1\\_managed-rag-kb-retrieve-generate-api.ipynb)\n",
    "- [2_customized-rag-retrieve-api-claude-v2.ipynb](./2\\_customized-rag-retrieve-api-claude-v2.ipynb)\n",
    "- [3_customized-rag-retrieve-api-langchain-claude-v2.ipynb](./3\\_customized-rag-retrieve-api-langchain-claude-v2.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "- create and delete Amazon IAM roles\n",
    "- create, update and delete Amazon S3 buckets\n",
    "- access Amazon Bedrock\n",
    "- access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "- IAMFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to enable `Anthropic Claude 3 Sonnet`  and `Anthropic Claude 3 Haiku` model access in Amazon Bedrock Console, as the notebook will use Anthropic Claude 3 Sonnet and Claude 3 Haiku models for testing the knowledge base once its created.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py==2.3.1\n",
      "  Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
      "                                              0.0/327.3 kB ? eta -:--:--\n",
      "     ---                                     30.7/327.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ----                                  41.0/327.3 kB 653.6 kB/s eta 0:00:01\n",
      "     -----------------------                204.8/327.3 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 327.3/327.3 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from opensearch-py==2.3.1) (1.26.18)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from opensearch-py==2.3.1) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from opensearch-py==2.3.1) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from opensearch-py==2.3.1) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.7)\n",
      "Installing collected packages: opensearch-py\n",
      "Successfully installed opensearch-py-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting retrying==1.3.4\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\yangd\\anaconda3\\lib\\site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Installing collected packages: retrying\n",
      "Successfully installed retrying-1.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U opensearch-py==2.3.1\n",
    "# # %pip install -U boto3==1.33.2\n",
    "# %pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个随机数作为后缀，范围在 200 到 900 之间，用于避免名称冲突\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "# 创建 STS（安全令牌服务）客户端，用于获取 AWS 账户相关信息\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# 创建一个 boto3 会话对象，用于与 AWS 服务交互\n",
    "boto3_session = boto3.session.Session()\n",
    "\n",
    "# 设置 AWS 区域名称为 \"us-west-2\"\n",
    "region_name = \"us-west-2\"\n",
    "# region_name = boto3_session.region_name # 也可以获取默认配置的区域名称（如果被注释掉）\n",
    "\n",
    "# 创建一个 Bedrock Agent 客户端，用于与 Bedrock Agent 服务交互\n",
    "# 使用我们指定的 region_name，即 'us-west-2'\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "\n",
    "# 定义要使用的 AWS 服务名，这里是 'aoss'（可能表示特定的服务，具体取决于实际业务）\n",
    "service = 'aoss'\n",
    "\n",
    "# 创建一个 S3 客户端，用于与 S3 服务交互\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# 使用 STS 客户端获取当前账户的账户 ID\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "# 生成 S3 后缀，由区域名称和账户 ID 组成，用于确保存储桶名称唯一\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "\n",
    "# 定义 S3 存储桶的名称，格式为 'bedrock-kb-<区域名称>-<账户 ID>'，确保唯一性\n",
    "# 需要根据具体需求替换为自己的存储桶名称\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}'\n",
    "\n",
    "# 创建一个 PrettyPrinter 对象，用于格式化输出（缩进为 2）\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "print(boto3_session.region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-west-2-590183949634 Exists\n"
     ]
    }
   ],
   "source": [
    "# 检查 S3 存储桶是否存在，如果不存在则创建一个用于知识库数据源的 S3 存储桶\n",
    "try:\n",
    "    # 调用 S3 客户端的 head_bucket 方法检查指定名称的存储桶是否存在\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    # 如果存储桶存在，输出存储桶名称和存在的信息\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    # 如果发生 ClientError 异常，表示存储桶不存在，接下来需要创建它\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    # 检查区域是否为 \"us-east-1\"，因为在这个区域创建 S3 存储桶不需要指定位置约束\n",
    "    if region_name == \"us-east-1\":\n",
    "        # 在 \"us-east-1\" 区域创建存储桶\n",
    "        s3bucket = s3_client.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        # 在其他区域创建存储桶时，需要指定位置约束为对应的区域名称\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region_name}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "First of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# 使用随机后缀生成向量存储名称和索引名称，确保唯一性\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "\n",
    "# 创建 OpenSearch Serverless 客户端，用于与 AWS OpenSearch 无服务器版本交互\n",
    "aoss_client = boto3_session.client('opensearchserverless', region_name=region_name)\n",
    "\n",
    "# 调用函数创建用于 Bedrock 的执行角色，并返回角色信息\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "\n",
    "# 获取执行角色的 ARN（Amazon Resource Name），以便后续使用\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create security, network and data access policies within OSS\n",
    "# encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "#                        aoss_client=aoss_client,\n",
    "#                        bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "# collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "# 创建安全、网络和数据访问策略，并将其应用于 OpenSearch Serverless\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(\n",
    "    vector_store_name=vector_store_name,\n",
    "    aoss_client=aoss_client,\n",
    "    bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn\n",
    ")\n",
    "\n",
    "# 在 OpenSearch Serverless 中创建一个名为 `vector_store_name` 的集合，用于向量搜索\n",
    "collection = aoss_client.create_collection(name=vector_store_name, type='VECTORSEARCH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Thu, 03 Oct 2024 12:37:39 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '99a3360a-e86d-4f8c-a221-d9ddf7d1c8ad'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '99a3360a-e86d-4f8c-a221-d9ddf7d1c8ad',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-west-2:590183949634:collection/51eh6pduvy17hrmiwb2g',\n",
      "                              'createdDate': 1727959059116,\n",
      "                              'id': '51eh6pduvy17hrmiwb2g',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1727959059116,\n",
      "                              'name': 'bedrock-sample-rag-568',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "# 将加密、网络、数据访问策略和集合信息存储为变量，便于后续操作\n",
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51eh6pduvy17hrmiwb2g.us-west-2.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# 获取 OpenSearch Serverless 集合的 URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "Creating collection...........\n",
      "Creating collection...........\n",
      "Creating collection...........\n",
      "Creating collection...........\n",
      "Creating collection...........\n",
      "..............................\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-west-2:590183949634:collection/51eh6pduvy17hrmiwb2g',\n",
      "    'collectionEndpoint': 'https://51eh6pduvy17hrmiwb2g.us-west-2.aoss.amazonaws.com',\n",
      "    'createdDate': 1727959059116,\n",
      "    'dashboardEndpoint': 'https://51eh6pduvy17hrmiwb2g.us-west-2.aoss.amazonaws.com/_dashboards',\n",
      "    'id': '51eh6pduvy17hrmiwb2g',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1727959295572,\n",
      "    'name': 'bedrock-sample-rag-568',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# 等待集合的创建完成\n",
    "# 集合的创建过程可能需要几分钟时间\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "\n",
    "# 定期检查集合的状态，直到状态变为“已创建”\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)  # 每隔 30 秒检查一次集合状态\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "\n",
    "# 集合创建成功，打印集合详情\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::590183949634:policy/AmazonBedrockOSSPolicyForKnowledgeBase_511\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# 创建 OpenSearch Serverless 访问策略并将其附加到 Bedrock 执行角色\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(\n",
    "        collection_id=collection_id,\n",
    "        bedrock_kb_execution_role=bedrock_kb_execution_role\n",
    "    )\n",
    "    # 数据访问规则的应用可能需要一分钟时间，因此等待 60 秒\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    # 如果策略已经存在，输出相关提示信息\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 在 OpenSearch Serverless 中创建向量索引，并设置 knn_vector 字段索引映射，指定维度大小、名称和引擎。\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()  # 获取 AWS 凭证\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)  # 使用 AWS 凭证和区域进行身份验证\n",
    "\n",
    "# 定义索引名称，确保名称唯一\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "\n",
    "# 定义索引配置，设置 KNN（近邻搜索）参数和字段映射\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",  # 启用 KNN 功能\n",
    "      \"number_of_shards\": 1,  # 设置分片数为 1\n",
    "      \"knn.algo_param.ef_search\": 512,  # 设置近邻搜索的参数，影响搜索速度和准确性\n",
    "      \"number_of_replicas\": 0,  # 设置副本数为 0，节省资源\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",  # 向量字段类型，使用 KNN 索引\n",
    "            \"dimension\": 1536,  # 向量维度为 1536\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",  # 使用 HNSW 算法\n",
    "                 \"engine\": \"faiss\",  # 使用 FAISS 引擎\n",
    "                 \"space_type\": \"l2\"  # 使用 L2 距离度量\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"  # 文本字段\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"  # 文本元数据字段\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# 构建 OpenSearch 客户端\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],  # 指定主机和端口\n",
    "    http_auth=awsauth,  # 使用 AWS V4 签名认证\n",
    "    use_ssl=True,  # 使用 SSL 进行安全连接\n",
    "    verify_certs=True,  # 验证 SSL 证书\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300  # 设置超时时间为 300 秒\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-568',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# 创建索引\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))  # 创建索引\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)  # 输出创建索引的响应\n",
    "\n",
    "    # 索引创建可能需要一分钟\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # 如果索引已经存在，可以选择删除现有索引\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to ingest into our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_root = \"./data/\"\n",
    "if not os.path.exists(data_root):\n",
    "    os.makedirs(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download to: ./data/AMZN-2022-Shareholder-Letter.pdf\n",
      "Attempting to download to: ./data/AMZN-2021-Shareholder-Letter.pdf\n",
      "Attempting to download to: ./data/AMZN-2020-Shareholder-Letter.pdf\n",
      "Attempting to download to: ./data/AMZN-2019-Shareholder-Letter.pdf\n"
     ]
    }
   ],
   "source": [
    "# # 下载并准备数据集\n",
    "# !mkdir -p ./data  # 创建用于存储数据的文件夹\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "# # 下载上述文件并保存到 data_root 目录下\n",
    "# for idx, url in enumerate(urls):\n",
    "#     file_path = data_root + filenames[idx]\n",
    "#     urlretrieve(url, file_path)\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = os.path.join(data_root, filenames[idx])\n",
    "    print(f\"Attempting to download to: {file_path}\")\n",
    "    urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 Bucket data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将数据上传到 S3 存储桶，作为知识库的数据源\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path, bucket_name):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            s3_client.upload_file(os.path.join(root, file), bucket_name, file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 配置 OpenSearch Serverless\n",
    "opensearchServerlessConfiguration = {\n",
    "    \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "    \"vectorIndexName\": index_name,\n",
    "    \"fieldMapping\": {\n",
    "        \"vectorField\": \"vector\",  # 向量字段名称\n",
    "        \"textField\": \"text\",  # 文本字段名称\n",
    "        \"metadataField\": \"text-metadata\"  # 元数据字段名称\n",
    "    }\n",
    "}\n",
    "\n",
    "# 配置数据分块策略 - 如何从数据源中分块导入数据\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,  # 每个块的最大 token 数量\n",
    "        \"overlapPercentage\": 20  # 分块时的重叠百分比，确保上下文连续性\n",
    "    }\n",
    "}\n",
    "\n",
    "# 配置数据源（S3）以将文档导入到 OpenSearch Serverless 知识库索引中\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",  # S3 存储桶的 ARN\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # 可选择使用此配置来限定从 S3 前缀中创建知识库\n",
    "}\n",
    "\n",
    "# 配置用于嵌入文档和实时提示的模型\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "# 知识库的名称和描述\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建知识库\n",
    "from retrying import retry\n",
    "\n",
    "# 使用 retry 装饰器，设置重试次数和等待时间\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    # 调用 bedrock_agent_client 创建知识库\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=roleArn,\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn  # 嵌入模型的 ARN\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 10, 3, 13, 26, 28, 134836, tzinfo=tzutc()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:590183949634:knowledge-base/UPMFSI0RZI',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'UPMFSI0RZI',\n",
      "  'name': 'bedrock-sample-knowledge-base-568',\n",
      "  'roleArn': 'arn:aws:iam::590183949634:role/AmazonBedrockExecutionRoleForKnowledgeBase_511',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-west-2:590183949634:collection/51eh6pduvy17hrmiwb2g',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-568'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 10, 3, 13, 26, 28, 134836, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 10, 3, 13, 30, 22, 957705, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-west-2-590183949634'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': '4DDMCZ5L6V',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'UPMFSI0RZI',\n",
      "  'name': 'bedrock-sample-knowledge-base-568',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 10, 3, 13, 30, 22, 957705, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8733fce9-462a-4276-a6ae-0861f5b44383',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 03 Oct 2024 13:30:28 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '8733fce9-462a-4276-a6ae-0861f5b44383',\n",
       "   'x-amz-apigw-id': 'fE1iPFwVvHcEWkA=',\n",
       "   'x-amzn-trace-id': 'Root=1-66fe9c74-7752c0085ce66ec544f69365'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'createdAt': datetime.datetime(2024, 10, 3, 13, 30, 22, 957705, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-west-2-590183949634'},\n",
       "   'type': 'S3'},\n",
       "  'dataSourceId': '4DDMCZ5L6V',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'knowledgeBaseId': 'UPMFSI0RZI',\n",
       "  'name': 'bedrock-sample-knowledge-base-568',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2024, 10, 3, 13, 30, 22, 957705, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\r"
     ]
    }
   ],
   "source": [
    "# Start an ingestion job\n",
    "interactive_sleep(30)\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': '4DDMCZ5L6V',\n",
      "  'ingestionJobId': 'DMLWQ5IDUV',\n",
      "  'knowledgeBaseId': 'UPMFSI0RZI',\n",
      "  'startedAt': datetime.datetime(2024, 10, 3, 13, 31, 55, 489639, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2024, 10, 3, 13, 31, 55, 489639, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': '4DDMCZ5L6V',\n",
      "  'ingestionJobId': 'DMLWQ5IDUV',\n",
      "  'knowledgeBaseId': 'UPMFSI0RZI',\n",
      "  'startedAt': datetime.datetime(2024, 10, 3, 13, 31, 55, 489639, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 10, 3, 13, 32, 11, 288177, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'UPMFSI0RZI'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Note: If you plan to run any following notebooks, you can skip this section\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"llama3-1-8b\", \"meta.llama3-1-8b-instruct-v1:0\"], [\"titan-text-lite\", \"amazon.titan-text-lite-v1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model ARN: arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-8b-instruct-v1:0\n",
      "Success!\n",
      "---------- Generated using llama3-1-8b:\n",
      "('Amazon is working on its own Large Language Models (LLMs) and has been '\n",
      " 'investing substantially in these models across all of its consumer, seller, '\n",
      " 'brand, and creator experiences. The company believes that LLMs and '\n",
      " 'Generative AI will transform and improve virtually every customer '\n",
      " 'experience. Amazon is also democratizing this technology so companies of all '\n",
      " 'sizes can leverage Generative AI through its AWS platform, which offers the '\n",
      " 'most price-performant machine learning chips in Trainium and Inferentia. '\n",
      " 'This allows companies to choose from various LLMs and build applications '\n",
      " 'with all of the AWS security, privacy, and other features that customers are '\n",
      " 'accustomed to using.')\n",
      "---------- The citations for the response generated by llama3-1-8b:\n",
      "[ 'Imagine what they’ll be able to do with reliable connectivity, from people '\n",
      "  'taking online education courses, using financial services, starting their '\n",
      "  'own businesses, doing their shopping, enjoying entertainment, to businesses '\n",
      "  'and governments improving their coverage, efficiency, and operations. '\n",
      "  'Kuiper will deliver not only accessibility, but affordability. Our teams '\n",
      "  'have developed low-cost antennas (i.e. customer terminals) that will lower '\n",
      "  'the barriers to access. We recently unveiled the new terminals that will '\n",
      "  'communicate with the satellites passing overhead, and we expect to be able '\n",
      "  'to produce our standard residential version for less than $400 each. '\n",
      "  'They’re small: 11 inches square, 1 inch thick, and weigh less than 5 pounds '\n",
      "  'without their mounting bracket, but they deliver speeds up to 400 megabits '\n",
      "  'per second. And they’re powered by Amazon-designed baseband chips. We’re '\n",
      "  'preparing to launch two prototype satellites to test the entire end-to-end '\n",
      "  'communications network this year, and plan to be in beta with commercial '\n",
      "  'customers in 2024. The customer reaction to what we’ve shared thus far '\n",
      "  'about Kuiper has been very positive, and we believe Kuiper represents a '\n",
      "  'very large potential opportunity for Amazon. It also shares several '\n",
      "  'similarities to AWS in that it’s capital intensive at the start, but has a '\n",
      "  'large prospective consumer, enterprise, and government customer base, '\n",
      "  'significant revenue and operating profit potential, and relatively few '\n",
      "  'companies with the technical and inventive aptitude, as well as the '\n",
      "  'investment hypothesis to go after it.   One final investment area that I’ll '\n",
      "  'mention, that’s core to setting Amazon up to invent in every area of our '\n",
      "  'business for many decades to come, and where we’re investing heavily is '\n",
      "  'Large Language Models (“LLMs”) and Generative AI. Machine learning has been '\n",
      "  'a technology with high promise for several decades, but it’s only been the '\n",
      "  'last five to ten years that it’s started to be used more pervasively by '\n",
      "  'companies. This shift was driven by several factors, including access to '\n",
      "  'higher volumes of compute capacity at lower prices than was ever available. '\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "Trying model ARN: arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-70b-instruct-v1:0\n",
      "Success!\n",
      "---------- Generated using llama3-1-70b:\n",
      "('Amazon has been working on its own Large Language Models (LLMs) for a while '\n",
      " 'now and believes that Generative AI will transform and improve virtually '\n",
      " 'every customer experience. They are investing substantially in these models '\n",
      " 'across all of their consumer, seller, brand, and creator experiences. Amazon '\n",
      " 'is also democratizing Generative AI technology so companies of all sizes can '\n",
      " 'leverage it. AWS is offering the most price-performant machine learning '\n",
      " 'chips, Trainium and Inferentia, so small and large companies can afford to '\n",
      " 'train and run their LLMs in production. Additionally, AWS is delivering '\n",
      " \"applications like AWS's CodeWhisperer, which revolutionizes developer \"\n",
      " 'productivity by generating code suggestions in real time.')\n",
      "---------- The citations for the response generated by llama3-1-70b:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    print(f\"Trying model ARN: {model_arn}\")\n",
    "    try:\n",
    "        response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "        print(\"Success!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with error: {str(e)}\")\n",
    "\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ { 'content': { 'text': 'Amazon has been using machine learning extensively '\n",
      "                         'for 25 years, employing it in everything from '\n",
      "                         'personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption. Generative AI '\n",
      "                         'is based on very Large Language Models (trained on '\n",
      "                         'up to hundreds of billions of parameters, and '\n",
      "                         'growing), across expansive datasets, and has '\n",
      "                         'radically general and broad recall and learning '\n",
      "                         'capabilities. We have been working on our own LLMs '\n",
      "                         'for a while now, believe it will transform and '\n",
      "                         'improve virtually every customer experience, and '\n",
      "                         'will continue to invest substantially in these '\n",
      "                         'models across all of our consumer, seller, brand, '\n",
      "                         'and creator experiences. Additionally, as we’ve done '\n",
      "                         'for years in AWS, we’re democratizing this '\n",
      "                         'technology so companies of all sizes can leverage '\n",
      "                         'Generative AI. AWS is offering the most '\n",
      "                         'price-performant machine learning chips in Trainium '\n",
      "                         'and Inferentia so small and large companies can '\n",
      "                         'afford to train and run their LLMs in production. We '\n",
      "                         'enable companies to choose from various LLMs and '\n",
      "                         'build applications with all of the AWS security, '\n",
      "                         'privacy and other features that customers are '\n",
      "                         'accustomed to using. And, we’re delivering '\n",
      "                         'applications like AWS’s CodeWhisperer, which '\n",
      "                         'revolutionizes        developer productivity by '\n",
      "                         'generating code suggestions in real time. I could '\n",
      "                         'write an entire letter on LLMs and Generative AI as '\n",
      "                         'I think they will be that transformative, but I’ll '\n",
      "                         'leave that for a future letter. Let’s just say that '\n",
      "                         'LLMs and Generative AI are going to be a big deal '\n",
      "                         'for customers, our shareholders, and Amazon.   So, '\n",
      "                         'in closing, I’m optimistic that we’ll emerge from '\n",
      "                         'this challenging macroeconomic time in a stronger '\n",
      "                         'position than when we entered it. There are several '\n",
      "                         'reasons for it and I’ve mentioned many of them '\n",
      "                         'above. But, there are two relatively simple '\n",
      "                         'statistics that underline our immense future '\n",
      "                         'opportunity. While we have a consumer business '\n",
      "                         'that’s $434B in 2022, the vast majority of total '\n",
      "                         'market segment share in global retail still resides '\n",
      "                         'in physical stores (roughly 80%). And, it’s a '\n",
      "                         'similar story for Global IT spending, where we have '\n",
      "                         'AWS revenue of $80B in 2022, with about 90% of '\n",
      "                         'Global IT spending still on-premises and yet to '\n",
      "                         'migrate to the cloud.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3ACg6UUpIBg69BNJEJpmlE',\n",
      "                  'x-amz-bedrock-kb-data-source-id': '4DDMCZ5L6V',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.6664225},\n",
      "  { 'content': { 'text': 'Our Inferentia2 chip, which just launched, offers up '\n",
      "                         'to four times higher throughput and ten times lower '\n",
      "                         'latency than our first Inferentia processor. With '\n",
      "                         'the enormous upcoming growth in machine learning, '\n",
      "                         'customers will be able to get a lot more done with '\n",
      "                         'AWS’s training and inference chips at a '\n",
      "                         'significantly lower cost. We’re not close to being '\n",
      "                         'done innovating here, and this long-term investment '\n",
      "                         'should prove fruitful for both customers and AWS. '\n",
      "                         'AWS is still in the early stages of its evolution, '\n",
      "                         'and has a chance for unusual growth in the next '\n",
      "                         'decade.   Similarly high potential, Amazon’s '\n",
      "                         'Advertising business is uniquely effective for '\n",
      "                         'brands, which is part of why it continues to grow at '\n",
      "                         'a brisk clip. Akin to physical retailers’ '\n",
      "                         'advertising businesses selling shelf space, end- '\n",
      "                         'caps, and placement in their circulars, our '\n",
      "                         'sponsored products and brands offerings have been an '\n",
      "                         'integral part        of the Amazon shopping '\n",
      "                         'experience for more than a decade. However, unlike '\n",
      "                         'physical retailers, Amazon can tailor these '\n",
      "                         'sponsored products to be relevant to what customers '\n",
      "                         'are searching for given what we know about shopping '\n",
      "                         'behaviors and our very deep investment in machine '\n",
      "                         'learning algorithms. This leads to advertising '\n",
      "                         'that’s more useful for customers; and as a result, '\n",
      "                         'performs better for brands. This is part of why our '\n",
      "                         'Advertising revenue has continued to grow rapidly '\n",
      "                         '(23% YoY in Q4 2022, 25% YoY overall for 2022 on a '\n",
      "                         '$31B revenue base), even as most large '\n",
      "                         'advertising-focused businesses’ growth have slowed '\n",
      "                         'over the last several quarters.   We strive to be '\n",
      "                         'the best place for advertisers to build their '\n",
      "                         'brands. We have near and long-term opportunities '\n",
      "                         'that will help us achieve that mission. We’re '\n",
      "                         'continuing to make large investments in machine '\n",
      "                         'learning to keep honing our advertising selection '\n",
      "                         'algorithms. For the past couple of years, we’ve '\n",
      "                         'invested in building comprehensive, flexible, and '\n",
      "                         'durable planning and measurement solutions, giving '\n",
      "                         'marketers greater insight into advertising '\n",
      "                         'effectiveness. An example is Amazon Marketing Cloud '\n",
      "                         '(“AMC”). AMC is a “clean room” (i.e. secure digital '\n",
      "                         'environment) in which advertisers can run custom '\n",
      "                         'audience and campaign analytics across a range of '\n",
      "                         'first and third-party inputs, in a privacy-safe '\n",
      "                         'manner, to generate advertising and business '\n",
      "                         'insights to inform their broader marketing and sales '\n",
      "                         'strategies. The Advertising and AWS teams have '\n",
      "                         'collaborated to enable companies to store their data '\n",
      "                         'in AWS, operate securely in AMC with Amazon and '\n",
      "                         'other third-party data sources, perform analytics in '\n",
      "                         'AWS, and have the option to activate advertising on '\n",
      "                         'Amazon or third-party publishers through the Amazon '\n",
      "                         'Demand-Side Platform.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AAw6UUpIBg69BNJEJpmlE',\n",
      "                  'x-amz-bedrock-kb-data-source-id': '4DDMCZ5L6V',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.54871696},\n",
      "  { 'content': { 'text': 'Imagine what they’ll be able to do with reliable '\n",
      "                         'connectivity, from people taking online education '\n",
      "                         'courses, using financial services, starting their '\n",
      "                         'own businesses, doing their shopping, enjoying '\n",
      "                         'entertainment, to businesses and governments '\n",
      "                         'improving their coverage, efficiency, and '\n",
      "                         'operations. Kuiper will deliver not only '\n",
      "                         'accessibility, but affordability. Our teams have '\n",
      "                         'developed low-cost antennas (i.e. customer '\n",
      "                         'terminals) that will lower the barriers to access. '\n",
      "                         'We recently unveiled the new terminals that will '\n",
      "                         'communicate with the satellites passing overhead, '\n",
      "                         'and we expect to be able to produce our standard '\n",
      "                         'residential version for less than $400 each. They’re '\n",
      "                         'small: 11 inches square, 1 inch thick, and weigh '\n",
      "                         'less than 5 pounds without their mounting bracket, '\n",
      "                         'but they deliver speeds up to 400 megabits per '\n",
      "                         'second. And they’re powered by Amazon-designed '\n",
      "                         'baseband chips. We’re preparing to launch two '\n",
      "                         'prototype satellites to test the entire end-to-end '\n",
      "                         'communications network this year, and plan to be in '\n",
      "                         'beta with commercial customers in 2024. The customer '\n",
      "                         'reaction to what we’ve shared thus far about Kuiper '\n",
      "                         'has been very positive, and we believe Kuiper '\n",
      "                         'represents a very large potential opportunity for '\n",
      "                         'Amazon. It also shares several similarities to AWS '\n",
      "                         'in that it’s capital intensive at the start, but has '\n",
      "                         'a large prospective consumer, enterprise, and '\n",
      "                         'government customer base, significant revenue and '\n",
      "                         'operating profit potential, and relatively few '\n",
      "                         'companies with the technical and inventive aptitude, '\n",
      "                         'as well as the investment hypothesis to go after '\n",
      "                         'it.   One final investment area that I’ll mention, '\n",
      "                         'that’s core to setting Amazon up to invent in every '\n",
      "                         'area of our business for many decades to come, and '\n",
      "                         'where we’re investing heavily is Large Language '\n",
      "                         'Models (“LLMs”) and Generative AI. Machine learning '\n",
      "                         'has been a technology with high promise for several '\n",
      "                         'decades, but it’s only been the last five to ten '\n",
      "                         'years that it’s started to be used more pervasively '\n",
      "                         'by companies. This shift was driven by several '\n",
      "                         'factors, including access to higher volumes of '\n",
      "                         'compute capacity at lower prices than was ever '\n",
      "                         'available. Amazon has been using machine learning '\n",
      "                         'extensively for 25 years, employing it in everything '\n",
      "                         'from personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3ACQ6UUpIBg69BNJEJpmlE',\n",
      "                  'x-amz-bedrock-kb-data-source-id': '4DDMCZ5L6V',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-west-2-590183949634/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.53047615}]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Next steps:</b> Proceed to the next labs to learn how to use Bedrock Knowledge bases. Remember to CLEAN_UP at the end of your session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "my_anaconda_env",
   "language": "python",
   "name": "my_anaconda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
